{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\iohkg\\miniconda3\\envs\\dl\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"fixed some syntax errors in the connector controller\"\n",
    "inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: fixed\n"
     ]
    }
   ],
   "source": [
    "outputs = model(**inputs)\n",
    "logits = outputs.logits\n",
    "important_sentences = torch.argmax(logits, dim=-1)\n",
    "\n",
    "# For demonstration, assume the first sentence is the summary\n",
    "summary = tokenizer.decode(inputs.input_ids[0][important_sentences])\n",
    "print(\"Summary:\", summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from scipy.spatial.distance import cosine\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get BERT embeddings\n",
    "def get_bert_embeddings(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state[0]\n",
    "\n",
    "# Function to calculate cosine similarity\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    return 1 - cosine(vec1, vec2)\n",
    "\n",
    "# Generate embeddings for a list of words\n",
    "def get_word_embeddings(word_list):\n",
    "    embeddings = {}\n",
    "    for word in word_list:\n",
    "        embeddings[word] = get_bert_embeddings(word).mean(dim=0).numpy()\n",
    "    return embeddings\n",
    "\n",
    "# Generate synonyms and related words\n",
    "def generate_synonyms_and_related(target_word, word_list, top_n=5):\n",
    "    word_embeddings = get_word_embeddings(word_list)\n",
    "    target_embedding = get_bert_embeddings(target_word).mean(dim=0).numpy()\n",
    "    \n",
    "    similarities = {word: cosine_similarity(target_embedding, emb) for word, emb in word_embeddings.items()}\n",
    "    sorted_similarities = sorted(similarities.items(), key=lambda item: item[1], reverse=True)\n",
    "    \n",
    "    return sorted_similarities[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "sentence = \"She felt happy about the results.\"\n",
    "target_word = \"happy\"\n",
    "word_list = [\"joyful\", \"pleased\", \"content\", \"delighted\", \"elated\", \"sad\", \"unhappy\", \"miserable\", \"dejected\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synonyms and related words for 'happy':\n",
      "pleased (similarity: 0.9016)\n",
      "sad (similarity: 0.8965)\n",
      "unhappy (similarity: 0.8582)\n",
      "miserable (similarity: 0.8540)\n",
      "delighted (similarity: 0.8236)\n"
     ]
    }
   ],
   "source": [
    "synonyms_and_related = generate_synonyms_and_related(target_word, word_list)\n",
    "print(f\"Synonyms and related words for '{target_word}':\")\n",
    "for word, similarity in synonyms_and_related:\n",
    "    print(f\"{word} (similarity: {similarity:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\iohkg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\iohkg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# Download WordNet data if not already downloaded\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_related_words(word):\n",
    "    synonyms = set()\n",
    "    for syn in wn.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.add(lemma.name())\n",
    "    return list(synonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Related words for 'implement': ['follow_through', 'carry_out', 'follow_up', 'implement', 'put_through', 'follow_out', 'go_through', 'apply', 'enforce']\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "target_word = \"implement\"\n",
    "related_words = get_related_words(target_word)\n",
    "print(f\"Related words for '{target_word}': {related_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\iohkg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\iohkg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GloVe model\n",
    "def load_glove_model(glove_file):\n",
    "    with open(glove_file, 'r', encoding='utf-8') as f:\n",
    "        model = {}\n",
    "        for line in f:\n",
    "            split_line = line.split()\n",
    "            word = split_line[0]\n",
    "            embedding = np.array([float(val) for val in split_line[1:]])\n",
    "            model[word] = embedding\n",
    "    return model\n",
    "\n",
    "# Function to calculate cosine similarity\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm1 = np.linalg.norm(vec1)\n",
    "    norm2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm1 * norm2)\n",
    "\n",
    "# Function to get related words using GloVe embeddings\n",
    "def get_related_words_glove(word, model, top_n=5):\n",
    "    if word not in model:\n",
    "        return f\"The word '{word}' is not in the vocabulary.\"\n",
    "    \n",
    "    word_embedding = model[word]\n",
    "    similarities = {}\n",
    "    \n",
    "    for other_word, other_embedding in model.items():\n",
    "        if other_word != word:\n",
    "            similarity = cosine_similarity(word_embedding, other_embedding)\n",
    "            similarities[other_word] = similarity\n",
    "    \n",
    "    sorted_similarities = sorted(similarities.items(), key=lambda item: item[1], reverse=True)\n",
    "    \n",
    "    return sorted_similarities[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the GloVe file\n",
    "glove_file = '../../../../../glove/glove.6B.300d.txt'\n",
    "\n",
    "# Load the GloVe model\n",
    "glove_model = load_glove_model(glove_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example sentence\n",
    "sentence = \"fixed some syntax errors in the connector controller\"\n",
    "\n",
    "# Example usage\n",
    "tokens = word_tokenize(sentence.lower())\n",
    "\n",
    "# Get related words for each token\n",
    "related_words_dict = {}\n",
    "for token in tokens:\n",
    "    related_words = get_related_words_glove(token, glove_model)\n",
    "    related_words_dict[token] = related_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fixed', 'some', 'syntax', 'errors', 'in', 'the', 'connector', 'controller']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Related words for 'fixed':\n",
      "  rate (similarity: 0.4819)\n",
      "  adjustable (similarity: 0.4738)\n",
      "  rates (similarity: 0.4600)\n",
      "  value (similarity: 0.4506)\n",
      "  variable (similarity: 0.4453)\n",
      "  basis (similarity: 0.4304)\n",
      "  minimum (similarity: 0.4297)\n",
      "  specified (similarity: 0.4261)\n",
      "  income (similarity: 0.4254)\n",
      "  investments (similarity: 0.4169)\n",
      "Related words for 'some':\n",
      "  many (similarity: 0.8490)\n",
      "  few (similarity: 0.8076)\n",
      "  those (similarity: 0.7384)\n",
      "  other (similarity: 0.7381)\n",
      "  have (similarity: 0.7370)\n",
      "  more (similarity: 0.7359)\n",
      "  several (similarity: 0.7230)\n",
      "  others (similarity: 0.7125)\n",
      "  these (similarity: 0.7073)\n",
      "  even (similarity: 0.7060)\n",
      "Related words for 'syntax':\n",
      "  semantics (similarity: 0.6332)\n",
      "  phonology (similarity: 0.5579)\n",
      "  codice_1 (similarity: 0.5574)\n",
      "  vocabulary (similarity: 0.5521)\n",
      "  xml (similarity: 0.5345)\n",
      "  morphology (similarity: 0.5337)\n",
      "  syntactic (similarity: 0.5309)\n",
      "  parsing (similarity: 0.5277)\n",
      "  punctuation (similarity: 0.5196)\n",
      "  html (similarity: 0.5195)\n",
      "Related words for 'errors':\n",
      "  error (similarity: 0.7476)\n",
      "  mistakes (similarity: 0.7350)\n",
      "  unforced (similarity: 0.7278)\n",
      "  faults (similarity: 0.5685)\n",
      "  mistake (similarity: 0.5404)\n",
      "  flaws (similarity: 0.5268)\n",
      "  lapses (similarity: 0.5139)\n",
      "  inaccuracies (similarity: 0.5103)\n",
      "  inconsistencies (similarity: 0.5082)\n",
      "  correct (similarity: 0.5035)\n",
      "Related words for 'in':\n",
      "  where (similarity: 0.6937)\n",
      "  last (similarity: 0.6580)\n",
      "  since (similarity: 0.6465)\n",
      "  the (similarity: 0.6361)\n",
      "  . (similarity: 0.6292)\n",
      "  same (similarity: 0.6290)\n",
      "  first (similarity: 0.6281)\n",
      "  , (similarity: 0.6157)\n",
      "  year (similarity: 0.6144)\n",
      "  which (similarity: 0.6093)\n",
      "Related words for 'the':\n",
      "  of (similarity: 0.7058)\n",
      "  which (similarity: 0.6992)\n",
      "  this (similarity: 0.6747)\n",
      "  part (similarity: 0.6727)\n",
      "  same (similarity: 0.6592)\n",
      "  its (similarity: 0.6447)\n",
      "  first (similarity: 0.6399)\n",
      "  in (similarity: 0.6361)\n",
      "  one (similarity: 0.6245)\n",
      "  that (similarity: 0.6176)\n",
      "Related words for 'connector':\n",
      "  connectors (similarity: 0.6975)\n",
      "  usb (similarity: 0.5435)\n",
      "  adapter (similarity: 0.5103)\n",
      "  adapters (similarity: 0.4717)\n",
      "  plugs (similarity: 0.4695)\n",
      "  coaxial (similarity: 0.4690)\n",
      "  interface (similarity: 0.4640)\n",
      "  connects (similarity: 0.4613)\n",
      "  motherboard (similarity: 0.4487)\n",
      "  firewire (similarity: 0.4450)\n",
      "Related words for 'controller':\n",
      "  controllers (similarity: 0.6377)\n",
      "  interface (similarity: 0.4397)\n",
      "  joystick (similarity: 0.4360)\n",
      "  westly (similarity: 0.4358)\n",
      "  i/o (similarity: 0.4161)\n",
      "  usb (similarity: 0.4117)\n",
      "  keyboard (similarity: 0.4112)\n",
      "  console (similarity: 0.4093)\n",
      "  recorder (similarity: 0.4022)\n",
      "  sixaxis (similarity: 0.4014)\n"
     ]
    }
   ],
   "source": [
    "# Print related words for each token\n",
    "for token, related_words in related_words_dict.items():\n",
    "    print(f\"Related words for '{token}':\")\n",
    "    for word, similarity in related_words:\n",
    "        print(f\"  {word} (similarity: {similarity:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "\n",
    "# Load the pre-trained BART model and tokenizer\n",
    "model_name = \"facebook/bart-large-cnn\"\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Function to summarize text\n",
    "def summarize_text(text, max_length=150, min_length=40, num_beams=4):\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer.encode(\"summarize: \" + text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    \n",
    "    # Generate summary\n",
    "    summary_ids = model.generate(inputs, max_length=max_length, min_length=min_length, num_beams=num_beams, length_penalty=2.0, early_stopping=True)\n",
    "    \n",
    "    # Decode the summary\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      "Fixed and updated some important features for the new versions\n",
      "\n",
      "Summary:\n",
      "Fixed and updated\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "text = \"Fixed and updated some important features for the new versions\"\n",
    "summary = summarize_text(text, 10, 10)[11:]\n",
    "print(\"Original Text:\")\n",
    "print(text)\n",
    "print(\"\\nSummary:\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "tokens = word_tokenize(summary.lower())\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [token for token in tokens if token not in stop_words and token.isalnum()]\n",
    "\n",
    "# Get related words for each token\n",
    "related_words_dict = {}\n",
    "for token in filtered_tokens:\n",
    "    related_words = get_related_words_glove(token, glove_model)\n",
    "    related_words_dict[token] = related_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Related words for 'fixed':\n",
      "  rate (similarity: 0.4819)\n",
      "  adjustable (similarity: 0.4738)\n",
      "  rates (similarity: 0.4600)\n",
      "  value (similarity: 0.4506)\n",
      "  variable (similarity: 0.4453)\n",
      "Related words for 'updated':\n",
      "  update (similarity: 0.7165)\n",
      "  updating (similarity: 0.6303)\n",
      "  revised (similarity: 0.5968)\n",
      "  updates (similarity: 0.5768)\n",
      "  version (similarity: 0.5056)\n"
     ]
    }
   ],
   "source": [
    "# Print related words for each token\n",
    "for token, related_words in related_words_dict.items():\n",
    "    print(f\"Related words for '{token}':\")\n",
    "    for word, similarity in related_words:\n",
    "        print(f\"  {word} (similarity: {similarity:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fixed': ['rate', 'adjustable', 'rates', 'value', 'variable'], 'updated': ['update', 'updating', 'revised', 'updates', 'version']}\n"
     ]
    }
   ],
   "source": [
    "related_words_only = {}\n",
    "for token, related_words in related_words_dict.items():\n",
    "    related_words_only[token] = []\n",
    "    for word, similarity in related_words:\n",
    "        related_words_only[token].append(word)\n",
    "print(related_words_only)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
