{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "import random\n",
    "import pandas as pd\n",
    "import related_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Revitalize and modernize a legacy software system', 'Develop a new product line that explores innovative technologies', 'Build a comprehensive data analytics and visualization platform', 'Integrate various software tools into a unified ecosystem', 'Create a secure, scalable cloud infrastructure', 'Develop a next-generation user interface for an existing application', 'Implement advanced cybersecurity measures across all products', 'Launch a comprehensive training and development platform for employees', 'Establish a centralized communication and collaboration hub', 'Create a real-time monitoring and feedback system for product performance'] ['gitlab', 'github', 'tableau', 'openclassroom', 'power bi', 'figma', 'draw.io', 'office', 'cisco'] ['clorinde', 'freminet', 'jean', 'emilie', 'kazuha', 'miko']\n"
     ]
    }
   ],
   "source": [
    "project_name_path = './projectName.txt'\n",
    "project_name_list = []\n",
    "with open(project_name_path, 'r') as file:\n",
    "    content = file.read()\n",
    "    project_name_list = content.split(\"\\n\")\n",
    "\n",
    "app_path = './app.txt'\n",
    "app_list = []\n",
    "with open(app_path, 'r') as file:\n",
    "    content = file.read()\n",
    "    app_list = content.split(\"\\n\")\n",
    "\n",
    "user_path = './user.txt'\n",
    "user_list = []\n",
    "with open(user_path, 'r') as file:\n",
    "    content = file.read()\n",
    "    user_list = content.split(\"\\n\")\n",
    "\n",
    "print(project_name_list, app_list, user_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules = {}\n",
    "rules[\"visual\"] = [\"tableau\", \"power bi\"]\n",
    "rules[\"monitor\"] = [\"tableau\", \"power bi\"]\n",
    "rules[\"software\"] = [\"gitlab\", \"github\"]\n",
    "rules[\"cloud\"] = [\"gitlab\", \"github\", \"cisco\"]\n",
    "rules[\"implement\"] = [\"gitlab\", \"github\"]\n",
    "rules[\"train\"] = [\"openclassroom\"]\n",
    "rules[\"hub\"] = [\"openclassroom\", \"cisco\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'visual': ['auditory', 'aural', 'imagery', 'cinematic', 'conceptual'], 'monitor': ['monitors', 'monitoring', 'monitored', 'supervise', 'assess'], 'software': ['computer', 'microsoft', 'hardware', 'computers', 'internet'], 'cloud': ['clouds', 'ash', 'shadow', 'smoke', 'dust'], 'implement': ['implementing', 'implemented', 'implementation', 'measures', 'enforce'], 'train': ['trains', 'bus', 'rail', 'commuter', 'freight'], 'hub': ['hubs', 'bustling', 'gateway', 'destinations', 'connecting']}\n"
     ]
    }
   ],
   "source": [
    "tokens = list(rules.keys())\n",
    "rules_extend = related_words.extract_related_words(tokens)\n",
    "print(rules_extend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = []\n",
    "\n",
    "# instead of random, we should have some rules !\n",
    "for _ in range(10000):\n",
    "    rand_hist = {}\n",
    "    rand_hist[\"processName\"] = random.choice(project_name_list)\n",
    "    triggered = False\n",
    "    for keyword in rules.keys():\n",
    "        if keyword in rand_hist[\"processName\"].lower():\n",
    "            triggered = True\n",
    "            rand_hist[\"app\"] = random.choice(rules[keyword])\n",
    "            break\n",
    "        for related_keyword in rules_extend[keyword]:\n",
    "            if related_keyword in rand_hist[\"processName\"].lower():\n",
    "                triggered = True\n",
    "                rand_hist[\"app\"] = random.choice(rules[keyword])\n",
    "                break\n",
    "        if triggered:\n",
    "            break\n",
    "            \n",
    "    if not triggered:\n",
    "        rand_hist[\"app\"] = random.choice(app_list)\n",
    "    rand_hist[\"userName\"] = random.choice(user_list)\n",
    "    hist.append(rand_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(hist) \n",
    "df.to_csv(\"hist.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\iohkg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\iohkg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example labeled reviews and their categories (labels)\n",
    "reviews = df[\"processName\"].to_list()\n",
    "labels = df[\"app\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess(review):\n",
    "    tokens = word_tokenize(review.lower())\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalnum() and token not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "processed_reviews = [preprocess(review) for review in reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text data to TF-IDF features\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(processed_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 44.45%\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Naive Bayes classifier\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vectorizer.joblib']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model and vectorizer\n",
    "joblib.dump(model, 'model.joblib')\n",
    "joblib.dump(vectorizer, 'vectorizer.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model and vectorizer\n",
    "loaded_model = joblib.load('model.joblib')\n",
    "loaded_vectorizer = joblib.load('vectorizer.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product suggestion: github\n"
     ]
    }
   ],
   "source": [
    "def suggest_product(review):\n",
    "    processed_review = preprocess(review)\n",
    "    review_tfidf = loaded_vectorizer.transform([processed_review])\n",
    "    prediction = loaded_model.predict(review_tfidf)[0]\n",
    "    return prediction\n",
    "\n",
    "# Example usage\n",
    "new_review = \"Generating a cybersecurity simulation\"\n",
    "suggestion = suggest_product(new_review)\n",
    "print(f\"Product suggestion: {suggestion}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
